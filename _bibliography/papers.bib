---
---

%@string{aps = {American Physical Society,}}

@inproceedings{he22_odyssey,
    bibtex_show = {true},
    selected = {true},
    author = {Jincheng He and Yuanyuan Bao and Na Xu and Hongfeng Li and Shicong Li and Linzhang Wang and Fei Xiang and Ming Li},
    title = {{Single-Channel Target Speaker Separation Using Joint Training with Target Speaker's Pitch Information}},
    year = 2022,
    booktitle = {Proc. The Speaker and Language Recognition Workshop (Odyssey 2022)},
    pages = {301--305},
    doi = {10.21437/Odyssey.2022-42},
    website = {https://www.isca-archive.org/odyssey_2022/he22_odyssey.html},
    pdf = {https://www.isca-archive.org/odyssey_2022/he22_odyssey.pdf},
    abstract = {Despite the great progress achieved in the target speaker separation (TSS) task, we are still trying to find other robust ways for performance improvement which are independent of the model architecture and the training loss. Pitch extraction plays an important role in many applications such as speech enhancement and speech separation. It is also a challenging task when there are multiple speakers in the same utterance. In this paper, we explore if the target speaker pitch extraction is possible and how the extracted target pitch could help to improve the TSS performance. A target pitch extraction model is built and incorporated into different TSS models using two different strategies, namely concatenation and joint training. The experimental results on the LibriSpeech dataset show that both training strategies could bring significant improvements to the TSS task, even the precision of the target pitch extraction module is not high enough.},
}

@misc{bao2021lightweight,
    bibtex_show = {true},
    title = {Lightweight Dual-channel Target Speaker Separation for Mobile Voice Communication},
    author = {Yuanyuan Bao and Yanze Xu and Na Xu and Wenjing Yang and Hongfeng Li and Shicong Li and Yongtao Jia and Fei Xiang and Jincheng He and Ming Li},
    year = {2021},
    eprint = {2106.02934},
    archivePrefix = {arXiv},
    primaryClass = {cs.SD},
    website = {https://arxiv.org/abs/2106.02934},
    pdf = {https://arxiv.org/pdf/2106.02934},
    abstract = {Nowadays, there is a strong need to deploy the target speaker separation (TSS) model on mobile devices with a limitation of the model size and computational complexity. To better perform TSS for mobile voice communication, we first make a dual-channel dataset based on a specific scenario, LibriPhone. Specifically, to better mimic the real-case scenario, instead of simulating from the single-channel dataset, LibriPhone is made by simultaneously replaying pairs of utterances from LibriSpeech by two professional artificial heads and recording by two built-in microphones of the mobile. Then, we propose a lightweight time-frequency domain separation model, LSTM-Former, which is based on the LSTM framework with source-to-noise ratio (SI-SNR) loss. For the experiments on Libri-Phone, we explore the dual-channel LSTMFormer model and a single-channel version by a random single channel of Libri-Phone. Experimental result shows that the dual-channel LSTM-Former outperforms the single-channel LSTMFormer with relative 25% improvement. This work provides a feasible solution for the TSS task on mobile devices, playing back and recording multiple data sources in real application scenarios for getting dual-channel real data can assist the lightweight model to achieve higher performance.},
}